# -*- coding: utf-8 -*-
"""Credit_Card_User_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kJvsrzglQw-fhKmckuKg1kyGzZ49sfYy
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
data = pd.read_csv('/content/drive/MyDrive/Credit_Card_Dataset.csv')
print("Jumlah data (baris, kolom):", data.shape)
print("\nInformasi tipe data dan skema pengkodean:")
print(data.info())
print("\nStatistik deskriptif untuk data numerik:")
print(data.describe())
print("\nBeberapa baris pertama data:")
print(data.head())
print("\nJumlah nilai unik untuk setiap kolom:")
for column in data.columns:
    print(f"{column}: {data[column].nunique()}")

print("\nJumlah nilai yang hilang per kolom:")
print(data.isnull().sum())

for col in data.select_dtypes(include=['object']).columns:
    print(f"\nNilai unik di kolom '{col}' (untuk memeriksa potensi data yang salah):")
    print(data[col].unique()[:50]) # Tampilkan hingga 50 nilai unik untuk ringkasan

print("\nMemeriksa potensi outlier melalui rentang nilai dalam statistik deskriptif:")
print(data.describe())

"""# Task
Perform detailed data exploration (EDA), data cleaning, feature engineering, data preprocessing, feature selection, and split the data into training, validation, and testing sets.

## Detailed data exploration (eda)

### Subtask:
Perform more in-depth analysis of the data through visualizations and statistical summaries to understand distributions, relationships, and potential issues.

**Reasoning**:
Create histograms and box plots for numerical columns to visualize their distributions and identify potential outliers.
"""

import matplotlib.pyplot as plt
import seaborn as sns

numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns

# Exclude Customer_ID and Defaulted from numerical columns for these plots
numerical_cols = numerical_cols.drop(['Defaulted'])

plt.figure(figsize=(15, 20))
for i, col in enumerate(numerical_cols):
    plt.subplot(len(numerical_cols), 2, 2 * i + 1)
    sns.histplot(data[col], kde=True)
    plt.title(f'Distribution of {col}')

    plt.subplot(len(numerical_cols), 2, 2 * i + 2)
    sns.boxplot(x=data[col])
    plt.title(f'Box plot of {col}')

plt.tight_layout()
plt.show()

"""**Reasoning**:
Create bar plots or count plots for categorical columns to understand the distribution of categories and analyze the distribution of the target variable 'Defaulted'.


"""

categorical_cols = data.select_dtypes(include=['object']).columns

plt.figure(figsize=(15, 10))
for i, col in enumerate(categorical_cols):
    plt.subplot(2, 3, i + 1)
    sns.countplot(x=data[col])
    plt.title(f'Distribution of {col}')
    plt.xticks(rotation=45, ha='right')

plt.subplot(2, 3, 6)
sns.countplot(x=data['Defaulted'])
plt.title('Distribution of Defaulted')

plt.tight_layout()
plt.show()

"""**Reasoning**:
Analyze the relationships between key numerical features using scatter plots and visualize correlations between numerical features using a heatmap.


"""

key_numerical_cols = ['Annual_Income', 'Credit_Score', 'Credit_Utilization_Ratio', 'Debt_To_Income_Ratio', 'Defaulted']
sns.pairplot(data[key_numerical_cols], hue='Defaulted')
plt.suptitle('Pair Plot of Key Numerical Features by Defaulted Status', y=1.02)
plt.show()

plt.figure(figsize=(12, 10))
correlation_matrix = data[numerical_cols].corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Numerical Features')
plt.show()

"""**Reasoning**:
Investigate the relationship between categorical features and the target variable using stacked bar plots or by examining group-wise statistics.


"""

for col in categorical_cols:
    if col != 'Customer_ID':
        plt.figure(figsize=(8, 5))
        sns.countplot(x=data[col], hue=data['Defaulted'])
        plt.title(f'Defaulted Status by {col}')
        plt.xticks(rotation=45, ha='right')
        plt.show()

for col in categorical_cols:
    if col != 'Customer_ID':
        print(f'\nDefaulted Status by {col}:\n')
        display(data.groupby(col)['Defaulted'].value_counts(normalize=True).unstack().fillna(0))

"""## Data cleaning

### Subtask:
Address any data quality issues identified during EDA, such as handling outliers or inconsistencies.

**Reasoning**:
Handle outliers in 'Annual_Income' and 'Credit_Utilization_Ratio' by capping them at the 99th percentile to mitigate the impact of extreme values. Then, visualize the distributions again to assess the effect of capping.
"""

# Cap outliers for 'Annual_Income' and 'Credit_Utilization_Ratio' at the 99th percentile
for col in ['Annual_Income', 'Credit_Utilization_Ratio']:
    upper_bound = data[col].quantile(0.99)
    data[col] = data[col].clip(upper=upper_bound)

# Re-examine distributions after outlier handling
plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
sns.boxplot(x=data['Annual_Income'])
plt.title('Box plot of Annual_Income after Capping')

plt.subplot(1, 2, 2)
sns.boxplot(x=data['Credit_Utilization_Ratio'])
plt.title('Box plot of Credit_Utilization_Ratio after Capping')

plt.tight_layout()
plt.show()

plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
sns.histplot(data['Annual_Income'], kde=True)
plt.title('Distribution of Annual_Income after Capping')

plt.subplot(1, 2, 2)
sns.histplot(data['Credit_Utilization_Ratio'], kde=True)
plt.title('Distribution of Credit_Utilization_Ratio after Capping')

plt.tight_layout()
plt.show()

"""## Feature engineering

### Subtask:
Create new features that could be beneficial for modeling based on the existing data and domain knowledge.

**Reasoning**:
Create the new features as described in the instructions, handling potential division by zero.
"""

import numpy as np

# 1. Create Income_Per_Transaction
# Handle division by zero by replacing 0 with a small value or NaN, then fill NaN
data['Income_Per_Transaction'] = data['Annual_Income'] / data['Total_Transactions_Last_Year'].replace(0, np.nan)
data['Income_Per_Transaction'].fillna(data['Income_Per_Transaction'].median(), inplace=True) # Fill with median

# 2. Create Avg_Monthly_Spend
data['Avg_Monthly_Spend'] = data['Total_Spend_Last_Year'] / 12

# 3. Create Late_Payment_Ratio
# Handle division by zero by replacing 0 with a small value or NaN, then fill NaN
data['Late_Payment_Ratio'] = data['Number_of_Late_Payments'] / data['Total_Transactions_Last_Year'].replace(0, np.nan)
data['Late_Payment_Ratio'].fillna(data['Late_Payment_Ratio'].median(), inplace=True) # Fill with median

# 4. Create interaction term (example: Credit_Score * Credit_Utilization_Ratio)
data['Credit_Score_x_Credit_Utilization'] = data['Credit_Score'] * data['Credit_Utilization_Ratio']

# 5. Create Age Groups (example: using cut)
data['Age_Group'] = pd.cut(data['Age'], bins=[20, 30, 40, 50, 60, 70], labels=['20-29', '30-39', '40-49', '50-59', '60-69'], right=False)

# Create Income Brackets (example: using qcut for equal frequency bins)
data['Income_Bracket'] = pd.qcut(data['Annual_Income'], q=5, labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])

# 6. The new features are added directly to the DataFrame above.

# 7. Display the first few rows and descriptive statistics of the DataFrame with new features
display(data.head())
display(data[['Income_Per_Transaction', 'Avg_Monthly_Spend', 'Late_Payment_Ratio', 'Credit_Score_x_Credit_Utilization', 'Age_Group', 'Income_Bracket']].describe(include='all'))

"""## Data preprocessing

### Subtask:
Prepare the data for machine learning models, including encoding categorical variables and scaling numerical features.

**Reasoning**:
Identify categorical and numerical columns, apply one-hot encoding to categorical columns and standard scaling to numerical columns, then concatenate the results with the target variable.
"""

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import pandas as pd

# Identify categorical and numerical columns, excluding 'Customer_ID' and 'Defaulted'
categorical_features = data.select_dtypes(include=['object', 'category']).columns.tolist()
# Ensure 'Customer_ID' is in the list before attempting to remove it
if 'Customer_ID' in categorical_features:
    categorical_features.remove('Customer_ID')

numerical_features = data.select_dtypes(include=['int64', 'float64']).columns.tolist()
# Ensure 'Defaulted' is in the list before attempting to remove it
if 'Defaulted' in numerical_features:
    numerical_features.remove('Defaulted')


# Create transformers for numerical and categorical features
numerical_transformer = StandardScaler()
categorical_transformer = OneHotEncoder(handle_unknown='ignore')

# Create a column transformer to apply different transformations to different columns
# Set remainder to 'drop' to explicitly drop columns not in numerical or categorical features
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ],
    remainder='drop' # Explicitly drop columns not specified
)

# Create a pipeline that first preprocesses
preprocessing_pipeline = Pipeline(steps=[('preprocessor', preprocessor)])

# Apply the preprocessing to the data
data_preprocessed_array = preprocessing_pipeline.fit_transform(data)

# Get the new column names after one-hot encoding
# This requires accessing the fitted OneHotEncoder from the pipeline
onehot_columns = preprocessing_pipeline.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(categorical_features)
all_preprocessed_columns = numerical_features + list(onehot_columns)

# Create a DataFrame from the preprocessed array
data_preprocessed = pd.DataFrame(data_preprocessed_array, columns=all_preprocessed_columns, index=data.index)

# Add the target variable back to the preprocessed DataFrame
data_preprocessed['Defaulted'] = data['Defaulted']

# Display the first few rows and column names of the resulting preprocessed DataFrame
display(data_preprocessed.head())
print("\nColumns in the preprocessed DataFrame:")
print(data_preprocessed.columns.tolist())

"""**Reasoning**:
The error indicates a mismatch between the shape of the preprocessed array and the number of columns inferred from the numerical and one-hot encoded features. This is likely because the `remainder='passthrough'` in the ColumnTransformer passed through the 'Customer_ID' column, which was not included in the `all_preprocessed_columns` list. The fix is to explicitly handle the 'Customer_ID' column by dropping it or including it correctly in the column list. I will drop it as it's not needed for modeling.


"""

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import pandas as pd

# Identify categorical and numerical columns, excluding 'Customer_ID' and 'Defaulted'
categorical_features = data.select_dtypes(include=['object', 'category']).columns.tolist()
# Ensure 'Customer_ID' is in the list before attempting to remove it
if 'Customer_ID' in categorical_features:
    categorical_features.remove('Customer_ID')

numerical_features = data.select_dtypes(include=['int64', 'float64']).columns.tolist()
# Ensure 'Defaulted' is in the list before attempting to remove it
if 'Defaulted' in numerical_features:
    numerical_features.remove('Defaulted')


# Create transformers for numerical and categorical features
numerical_transformer = StandardScaler()
categorical_transformer = OneHotEncoder(handle_unknown='ignore')

# Create a column transformer to apply different transformations to different columns
# Set remainder to 'drop' to explicitly drop columns not in numerical or categorical features
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ],
    remainder='drop' # Explicitly drop columns not specified
)

# Create a pipeline that first preprocesses
preprocessing_pipeline = Pipeline(steps=[('preprocessor', preprocessor)])

# Apply the preprocessing to the data
data_preprocessed_array = preprocessing_pipeline.fit_transform(data)

# Get the new column names after one-hot encoding
# This requires accessing the fitted OneHotEncoder from the pipeline
onehot_columns = preprocessing_pipeline.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(categorical_features)
all_preprocessed_columns = numerical_features + list(onehot_columns)

# Create a DataFrame from the preprocessed array
data_preprocessed = pd.DataFrame(data_preprocessed_array, columns=all_preprocessed_columns, index=data.index)

# Add the target variable back to the preprocessed DataFrame
data_preprocessed['Defaulted'] = data['Defaulted']

# Display the first few rows and column names of the resulting preprocessed DataFrame
display(data_preprocessed.head())
print("\nColumns in the preprocessed DataFrame:")
print(data_preprocessed.columns.tolist())

"""## Feature selection

### Subtask:
Select the most relevant features for modeling to improve performance and reduce complexity.

**Reasoning**:
Separate features and target, calculate correlations, display absolute correlations in descending order, and select features based on correlation.
"""

import pandas as pd
from sklearn.feature_selection import SelectKBest, f_classif

# Separate features (X) and target (y)
X = data_preprocessed.drop('Defaulted', axis=1)
y = data_preprocessed['Defaulted']

# Calculate correlation of each feature with the target variable
correlations = X.corrwith(y)

# Display the absolute values of these correlations in descending order
print("Feature correlations with Defaulted (absolute value, descending):")
print(correlations.abs().sort_values(ascending=False))

# Use SelectKBest to select the top k features based on f-classification
# Let's choose k=15 as an example based on potential higher correlations
selector = SelectKBest(score_func=f_classif, k=15)
X_selected = selector.fit_transform(X, y)

# Get the names of the selected features
selected_feature_indices = selector.get_support(indices=True)
selected_features = X.columns[selected_feature_indices].tolist()

print("\nSelected features based on SelectKBest (k=15):")
print(selected_features)

# Store the selected features in a new DataFrame for clarity, although the list is sufficient for modeling
X_selected_df = X[selected_features]

# Display the first few rows of the selected features DataFrame
display(X_selected_df.head())

"""## Splitting the data

### Subtask:
Split the dataset into training, validation, and testing sets.

**Reasoning**:
Import the necessary function and split the data into training, testing, and validation sets according to the instructions. Then, print the shapes to verify the split.
"""

from sklearn.model_selection import train_test_split

# Split data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X_selected_df, y, test_size=0.2, random_state=42)

# Split the training data into training and validation sets (80% train, 20% validation)
# This results in the original train set being split into 80% new train and 20% validation.
# The sizes relative to the original dataset are: Train: 0.8 * 0.8 = 0.64, Validation: 0.8 * 0.2 = 0.16, Test: 0.2
X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

# Print the shapes of the resulting sets
print("Shape of X_train_split:", X_train_split.shape)
print("Shape of X_val:", X_val.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of y_train_split:", y_train_split.shape)
print("Shape of y_val:", y_val.shape)
print("Shape of y_test:", y_test.shape)

"""## Summary:

### Data Analysis Key Findings

*   Distributions of numerical features like `Annual_Income` and `Credit_Utilization_Ratio` showed potential outliers.
*   The target variable `Defaulted` exhibited a slight class imbalance, with non-defaults being more frequent.
*   Pair plots suggest some separation between default and non-default classes based on `Credit_Score` and `Credit_Utilization_Ratio`.
*   Correlation analysis revealed weak correlations among most numerical features.
*   Categorical features showed similar default rates across their categories.
*   Outliers in `Annual_Income` and `Credit_Utilization_Ratio` were addressed by capping values at the 99th percentile.
*   New features such as `Income_Per_Transaction`, `Avg_Monthly_Spend`, `Late_Payment_Ratio`, an interaction term (`Credit_Score_x_Credit_Utilization`), `Age_Group`, and `Income_Bracket` were successfully created.
*   Numerical features were scaled using `StandardScaler`, and categorical features were one-hot encoded using `OneHotEncoder`.
*   Feature selection using `SelectKBest` with `f_classif` identified the top 15 features, including those with slightly higher correlations and others.
*   The dataset was successfully split into training (64%), validation (16%), and testing (20%) sets.

### Insights or Next Steps

*   The relatively weak correlations observed suggest that complex feature interactions or non-linear relationships might be important for predicting default.
*   Given the slight class imbalance, consider using evaluation metrics beyond simple accuracy (e.g., precision, recall, F1-score, AUC) and potentially explore techniques like oversampling or undersampling during model training.

# Task
Train and evaluate multiple classification models (Logistic Regression, Random Forest, Gradient Boosting) on the provided dataset, select the best performing model based on evaluation metrics, and report its performance on the test set.

## Train a logistic regression model

### Subtask:
Train a Logistic Regression model on the training data.

**Reasoning**:
Train a Logistic Regression model on the training data.
"""

from sklearn.linear_model import LogisticRegression

# Instantiate a Logistic Regression model
log_reg_model = LogisticRegression()

# Train the model on the training data
log_reg_model.fit(X_train_split, y_train_split)

"""## Train a random forest model

### Subtask:
Train a Random Forest model on the training data.

**Reasoning**:
Import the necessary class, instantiate the model, and train it on the training data.
"""

from sklearn.ensemble import RandomForestClassifier

# Instantiate a Random Forest model
rf_model = RandomForestClassifier(random_state=42)

# Train the model on the training data
rf_model.fit(X_train_split, y_train_split)

"""## Train a gradient boosting model

### Subtask:
Train a Gradient Boosting model on the training data.

**Reasoning**:
Train a Gradient Boosting model on the training data.
"""

from sklearn.ensemble import GradientBoostingClassifier

# Instantiate a Gradient Boosting model
gbm_model = GradientBoostingClassifier(random_state=42)

# Train the model on the training data
gbm_model.fit(X_train_split, y_train_split)

"""## Evaluate the models

### Subtask:
Evaluate the trained models on the validation set using appropriate metrics for binary classification (e.g., accuracy, precision, recall, F1-score, AUC).

**Reasoning**:
Evaluate the trained models on the validation set using various classification metrics to compare their performance.
"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Dictionary to store evaluation results
evaluation_results = {}

# Evaluate Logistic Regression model
y_val_pred_log_reg = log_reg_model.predict(X_val)
y_val_proba_log_reg = log_reg_model.predict_proba(X_val)[:, 1] # Get probability of positive class

evaluation_results['Logistic Regression'] = {
    'Accuracy': accuracy_score(y_val, y_val_pred_log_reg),
    'Precision': precision_score(y_val, y_val_pred_log_reg),
    'Recall': recall_score(y_val, y_val_pred_log_reg),
    'F1-score': f1_score(y_val, y_val_pred_log_reg),
    'AUC': roc_auc_score(y_val, y_val_proba_log_reg)
}

# Evaluate Random Forest model
y_val_pred_rf = rf_model.predict(X_val)
y_val_proba_rf = rf_model.predict_proba(X_val)[:, 1] # Get probability of positive class

evaluation_results['Random Forest'] = {
    'Accuracy': accuracy_score(y_val, y_val_pred_rf),
    'Precision': precision_score(y_val, y_val_pred_rf),
    'Recall': recall_score(y_val, y_val_pred_rf),
    'F1-score': f1_score(y_val, y_val_pred_rf),
    'AUC': roc_auc_score(y_val, y_val_proba_rf)
}

# Evaluate Gradient Boosting model
y_val_pred_gbm = gbm_model.predict(X_val)
y_val_proba_gbm = gbm_model.predict_proba(X_val)[:, 1] # Get probability of positive class

evaluation_results['Gradient Boosting'] = {
    'Accuracy': accuracy_score(y_val, y_val_pred_gbm),
    'Precision': precision_score(y_val, y_val_pred_gbm),
    'Recall': recall_score(y_val, y_val_pred_gbm),
    'F1-score': f1_score(y_val, y_val_pred_gbm),
    'AUC': roc_auc_score(y_val, y_val_proba_gbm)
}

# Display the evaluation results
print("Model Evaluation Results on Validation Set:")
for model_name, metrics in evaluation_results.items():
    print(f"\n{model_name}:")
    for metric_name, value in metrics.items():
        print(f"  {metric_name}: {value:.4f}")

"""## Select the best model

### Subtask:
Based on the evaluation metrics from the previous step, select the best performing model.

**Reasoning**:
Compare the AUC scores from the evaluation results to select the best performing model and store it in `best_model`.
"""

# Find the model with the highest AUC score on the validation set
best_model_name = None
best_auc_score = -1

for model_name, metrics in evaluation_results.items():
    if metrics['AUC'] > best_auc_score:
        best_auc_score = metrics['AUC']
        best_model_name = model_name

# Assign the best model object based on the best_model_name
if best_model_name == 'Logistic Regression':
    best_model = log_reg_model
elif best_model_name == 'Random Forest':
    best_model = rf_model
elif best_model_name == 'Gradient Boosting':
    best_model = gbm_model
else:
    best_model = None # Handle cases where no model is selected

# Print the name of the selected best model
print(f"The best performing model on the validation set is: {best_model_name}")

"""## Evaluate the best model on the test set

### Subtask:
Evaluate the selected best model on the unseen test set to get a final performance estimate.

**Reasoning**:
Evaluate the selected best model on the unseen test set to get a final performance estimate by predicting labels and probabilities and calculating evaluation metrics.
"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Predict class labels on the test set
y_test_pred = best_model.predict(X_test)

# Predict probabilities of the positive class on the test set
y_test_proba = best_model.predict_proba(X_test)[:, 1]

# Calculate evaluation metrics on the test set
accuracy = accuracy_score(y_test, y_test_pred)
precision = precision_score(y_test, y_test_pred)
recall = recall_score(y_test, y_test_pred)
f1 = f1_score(y_test, y_test_pred)
auc = roc_auc_score(y_test, y_test_proba)

# Print the evaluation metrics
print("Best Model Performance on Test Set:")
print(f"  Accuracy: {accuracy:.4f}")
print(f"  Precision: {precision:.4f}")
print(f"  Recall: {recall:.4f}")
print(f"  F1-score: {f1:.4f}")
print(f"  AUC: {auc:.4f}")

"""## Summary:

### Q&A
**What is the best performing model based on the evaluation metrics?**

Based on the evaluation metrics from the validation set, the Logistic Regression model was selected as the best performing model.

**What is the performance of the best model on the test set?**

The performance of the best model (Logistic Regression) on the test set is as follows:
- Accuracy: 0.6735
- Precision: 0.5318
- Recall: 0.3324
- F1-score: 0.4090
- AUC: 0.6827

### Data Analysis Key Findings
* Three classification models (Logistic Regression, Random Forest, and Gradient Boosting) were trained and evaluated on the dataset.
* The Logistic Regression model was identified as the best performing model based on its superior performance on the validation set, particularly in terms of the AUC score.
* The selected Logistic Regression model achieved an accuracy of 67.35%, a precision of 53.18%, a recall of 33.24%, an F1-score of 40.90%, and an AUC of 68.27% on the unseen test set.

### Insights or Next Steps
* Given the relatively low recall and F1-score, further investigation into techniques for handling class imbalance or feature engineering might be beneficial to improve the model's ability to correctly identify positive cases.
* Exploring hyperparameter tuning for the Logistic Regression model could potentially lead to further performance improvements.

"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
data = pd.read_csv('/content/drive/MyDrive/Credit_Card_Dataset.csv')
print("Jumlah data (baris, kolom):", data.shape)
print("\nInformasi tipe data dan skema pengkodean:")
print(data.info())
print("\nStatistik deskriptif untuk data numerik:")
print(data.describe())
print("\nBeberapa baris pertama data:")
print(data.head())
print("\nJumlah nilai unik untuk setiap kolom:")
for column in data.columns:
    print(f"{column}: {data[column].nunique()}")

print("\nJumlah nilai yang hilang per kolom:")
print(data.isnull().sum())

for col in data.select_dtypes(include=['object']).columns:
    print(f"\nNilai unik di kolom '{col}' (untuk memeriksa potensi data yang salah):")
    print(data[col].unique()[:50]) # Tampilkan hingga 50 nilai unik untuk ringkasan

print("\nMemeriksa potensi outlier melalui rentang nilai dalam statistik deskriptif:")
print(data.describe())

import matplotlib.pyplot as plt
import seaborn as sns

numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns

# Exclude Customer_ID and Defaulted from numerical columns for these plots
numerical_cols = numerical_cols.drop(['Defaulted'])

plt.figure(figsize=(15, 20))
for i, col in enumerate(numerical_cols):
    plt.subplot(len(numerical_cols), 2, 2 * i + 1)
    sns.histplot(data[col], kde=True)
    plt.title(f'Distribution of {col}')

    plt.subplot(len(numerical_cols), 2, 2 * i + 2)
    sns.boxplot(x=data[col])
    plt.title(f'Box plot of {col}')

plt.tight_layout()
plt.show()

categorical_cols = data.select_dtypes(include=['object']).columns

plt.figure(figsize=(15, 10))
for i, col in enumerate(categorical_cols):
    plt.subplot(2, 3, i + 1)
    sns.countplot(x=data[col])
    plt.title(f'Distribution of {col}')
    plt.xticks(rotation=45, ha='right')

plt.subplot(2, 3, 6)
sns.countplot(x=data['Defaulted'])
plt.title('Distribution of Defaulted')

plt.tight_layout()
plt.show()

key_numerical_cols = ['Annual_Income', 'Credit_Score', 'Credit_Utilization_Ratio', 'Debt_To_Income_Ratio', 'Defaulted']
sns.pairplot(data[key_numerical_cols], hue='Defaulted')
plt.suptitle('Pair Plot of Key Numerical Features by Defaulted Status', y=1.02)
plt.show()

plt.figure(figsize=(12, 10))
correlation_matrix = data[numerical_cols].corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Numerical Features')
plt.show()

for col in categorical_cols:
    if col != 'Customer_ID':
        plt.figure(figsize=(8, 5))
        sns.countplot(x=data[col], hue=data['Defaulted'])
        plt.title(f'Defaulted Status by {col}')
        plt.xticks(rotation=45, ha='right')
        plt.show()

for col in categorical_cols:
    if col != 'Customer_ID':
        print(f'\nDefaulted Status by {col}:\n')
        display(data.groupby(col)['Defaulted'].value_counts(normalize=True).unstack().fillna(0))

import matplotlib.pyplot as plt
import seaborn as sns

# Cap outliers for 'Annual_Income' and 'Credit_Utilization_Ratio' at the 99th percentile
for col in ['Annual_Income', 'Credit_Utilization_Ratio']:
    upper_bound = data[col].quantile(0.99)
    data[col] = data[col].clip(upper=upper_bound)

# Re-examine distributions after outlier handling
plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
sns.boxplot(x=data['Annual_Income'])
plt.title('Box plot of Annual_Income after Capping')

plt.subplot(1, 2, 2)
sns.boxplot(x=data['Credit_Utilization_Ratio'])
plt.title('Box plot of Credit_Utilization_Ratio after Capping')

plt.tight_layout()
plt.show()

plt.figure(figsize=(15, 5))

plt.subplot(1, 2, 1)
sns.histplot(data['Annual_Income'], kde=True)
plt.title('Distribution of Annual_Income after Capping')

plt.subplot(1, 2, 2)
sns.histplot(data['Credit_Utilization_Ratio'], kde=True)
plt.title('Distribution of Credit_Utilization_Ratio after Capping')

plt.tight_layout()
plt.show()

import numpy as np

# 1. Create Income_Per_Transaction
# Handle division by zero by replacing 0 with a small value or NaN, then fill NaN
data['Income_Per_Transaction'] = data['Annual_Income'] / data['Total_Transactions_Last_Year'].replace(0, np.nan)
data['Income_Per_Transaction'].fillna(data['Income_Per_Transaction'].median(), inplace=True) # Fill with median

# 2. Create Avg_Monthly_Spend
data['Avg_Monthly_Spend'] = data['Total_Spend_Last_Year'] / 12

# 3. Create Late_Payment_Ratio
# Handle division by zero by replacing 0 with a small value or NaN, then fill NaN
data['Late_Payment_Ratio'] = data['Number_of_Late_Payments'] / data['Total_Transactions_Last_Year'].replace(0, np.nan)
data['Late_Payment_Ratio'].fillna(data['Late_Payment_Ratio'].median(), inplace=True) # Fill with median

# 4. Create interaction term (example: Credit_Score * Credit_Utilization_Ratio)
data['Credit_Score_x_Credit_Utilization'] = data['Credit_Score'] * data['Credit_Utilization_Ratio']

# 5. Create Age Groups (example: using cut)
data['Age_Group'] = pd.cut(data['Age'], bins=[20, 30, 40, 50, 60, 70], labels=['20-29', '30-39', '40-49', '50-59', '60-69'], right=False)

# Create Income Brackets (example: using qcut for equal frequency bins)
data['Income_Bracket'] = pd.qcut(data['Annual_Income'], q=5, labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])

# 6. The new features are added directly to the DataFrame above.

# 7. Display the first few rows and descriptive statistics of the DataFrame with new features
display(data.head())
display(data[['Income_Per_Transaction', 'Avg_Monthly_Spend', 'Late_Payment_Ratio', 'Credit_Score_x_Credit_Utilization', 'Age_Group', 'Income_Bracket']].describe(include='all'))

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import pandas as pd

# Identify categorical and numerical columns, excluding 'Customer_ID' and 'Defaulted'
categorical_features = data.select_dtypes(include=['object', 'category']).columns.tolist()
# Ensure 'Customer_ID' is in the list before attempting to remove it
if 'Customer_ID' in categorical_features:
    categorical_features.remove('Customer_ID')

numerical_features = data.select_dtypes(include=['int64', 'float64']).columns.tolist()
# Ensure 'Defaulted' is in the list before attempting to remove it
if 'Defaulted' in numerical_features:
    numerical_features.remove('Defaulted')


# Create transformers for numerical and categorical features
numerical_transformer = StandardScaler()
categorical_transformer = OneHotEncoder(handle_unknown='ignore')

# Create a column transformer to apply different transformations to different columns
# Set remainder to 'drop' to explicitly drop columns not in numerical or categorical features
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ],
    remainder='drop' # Explicitly drop columns not specified
)

# Create a pipeline that first preprocesses
preprocessing_pipeline = Pipeline(steps=[('preprocessor', preprocessor)])

# Apply the preprocessing to the data
data_preprocessed_array = preprocessing_pipeline.fit_transform(data)

# Get the new column names after one-hot encoding
# This requires accessing the fitted OneHotEncoder from the pipeline
onehot_columns = preprocessing_pipeline.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(categorical_features)
all_preprocessed_columns = numerical_features + list(onehot_columns)

# Create a DataFrame from the preprocessed array
data_preprocessed = pd.DataFrame(data_preprocessed_array, columns=all_preprocessed_columns, index=data.index)

# Add the target variable back to the preprocessed DataFrame
data_preprocessed['Defaulted'] = data['Defaulted']

# Display the first few rows and column names of the resulting preprocessed DataFrame
display(data_preprocessed.head())
print("\nColumns in the preprocessed DataFrame:")
print(data_preprocessed.columns.tolist())

import pandas as pd
from sklearn.feature_selection import SelectKBest, f_classif

# Separate features (X) and target (y)
X = data_preprocessed.drop('Defaulted', axis=1)
y = data_preprocessed['Defaulted']

# Calculate correlation of each feature with the target variable
correlations = X.corrwith(y)

# Display the absolute values of these correlations in descending order
print("Feature correlations with Defaulted (absolute value, descending):")
print(correlations.abs().sort_values(ascending=False))

# Use SelectKBest to select the top k features based on f-classification
# Let's choose k=15 as an example based on potential higher correlations
selector = SelectKBest(score_func=f_classif, k=15)
X_selected = selector.fit_transform(X, y)

# Get the names of the selected features
selected_feature_indices = selector.get_support(indices=True)
selected_features = X.columns[selected_feature_indices].tolist()

print("\nSelected features based on SelectKBest (k=15):")
print(selected_features)

# Store the selected features in a new DataFrame for clarity, although the list is sufficient for modeling
X_selected_df = X[selected_features]

# Display the first few rows of the selected features DataFrame
display(X_selected_df.head())

# Identifikasi Parameter untuk Model

print("Parameter yang mungkin relevan untuk model:")
print("- Fitur terpilih: ", selected_features)
print("  - Parameter ini digunakan sebagai input untuk model, dipilih berdasarkan skor ANOVA F-value.")
print("- Target label: 'Defaulted'") # Corrected target label
print("  - Parameter ini merupakan output yang ingin diprediksi oleh model, mewakili apakah customer akan Defaulted atau tidak.") # Corrected explanation
print("- Data setelah normalisasi dan pembersihan.")
print("  - Data yang telah dibersihkan dan dinormalisasi diharapkan meningkatkan performa model.")

# Catatan:
# - Parameter-parameter lain yang mungkin relevan bergantung pada model yang akan digunakan.
# - Misalnya, jika menggunakan model pohon keputusan, parameter seperti kedalaman pohon dan jumlah daun bisa dipertimbangkan.
# - Jika menggunakan model regresi logistik, parameter seperti regularization dan learning rate bisa dipertimbangkan.
# - Selanjutnya, Anda dapat melakukan analisis lebih lanjut terkait parameter model yang paling optimal untuk dipilih berdasarkan model yang diinginkan dan dataset yang dimiliki.